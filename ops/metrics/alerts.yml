groups:
- name: convexa.alerts
  rules:
  - alert: HighErrorRate
    expr: sum(rate(http_requests_total{status=~"5..", job="convexa"}[5m])) / sum(rate(http_requests_total{job="convexa"}[5m])) > 0.02
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "High HTTP 5xx error rate detected"
      description: "The HTTP 5xx error rate has been above 2% for 5 minutes. Current value: {{ $value | humanizePercentage }}"

  - alert: HighLatency
    expr: histogram_quantile(0.95, sum by(le, path) (rate(http_request_duration_seconds_bucket{job="convexa"}[5m]))) > 0.5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High p95 HTTP request latency detected"
      description: "The p95 HTTP request latency has been above 500ms for 5 minutes. Current value: {{ $value }}s"

  - alert: WebhookErrorsGrowth
    expr: rate(webhook_errors_total[5m]) > 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Webhook errors are growing"
      description: "Webhook errors are increasing. Current rate: {{ $value }} errors/sec"

  - alert: DialDispositionStall
    expr: (time() - max(dialer_disposition_total_timestamp)) > 300
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Dial disposition metric stalled"
      description: "The dialer_disposition_total metric has not been updated for more than 5 minutes."

  - alert: RedisDown
    expr: redis_up == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Redis is down"
      description: "Redis instance is not reachable."

  - alert: DiskUsageHigh
    expr: node_disk_utilization > 0.8
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Disk usage is high"
      description: "Disk usage is above 80%. Current value: {{ $value | humanizePercentage }}"

  - alert: QuotaApproachingLimit
    expr: convexa_quota_fraction{resource="attom_api"} > 0.8
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "ATTOM API quota approaching limit"
      description: "The ATTOM API quota usage is at {{ $value | humanizePercentage }}. Consider increasing the daily cap or investigating usage patterns."

  - alert: QuotaHardStop
    expr: convexa_quota_fraction{resource="attom_api"} >= 1
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "ATTOM API quota hard stop reached"
      description: "The ATTOM API quota has reached 100%. Demo mode fallback should be active."

  - alert: NoCampaignQueries
    expr: sum(rate(campaign_queries_total[15m])) == 0
    for: 15m
    labels:
      severity: critical
    annotations:
      summary: "No campaign queries observed"
      description: "No campaign queries have been observed for 15 minutes. This might indicate an issue with campaign processing. TODO: Enhance with check for non-zero in prior 24h."

  - alert: StaleLeadGrades
    expr: changes(lead_grades_total[24h]) == 0
    for: 24h
    labels:
      severity: critical
    annotations:
      summary: "Lead grades not changing"
      description: "The lead_grades_total metric has not changed for 24 hours, indicating stale lead grading jobs."

  - alert: NoDispositionsToday
    expr: sum(increase(dialer_disposition_total[24h])) == 0
    for: 24h
    labels:
      severity: critical
    annotations:
      summary: "No dispositions recorded today"
      description: "No dial dispositions have been recorded in the last 24 hours. This might indicate an issue with dialer operations."

  - alert: OverdueFollowupsSpike
    expr: pi1_followups_overdue > 10
    for: 30m
    labels:
      severity: warning
    annotations:
      summary: "Spike in overdue follow-ups"
      description: "The number of overdue follow-ups has been above 10 for 30 minutes. Current count: {{ $value }}"

  - alert: StaleTimeline
    expr: sum(increase(timeline_events_total[2h])) == 0
    for: 2h
    labels:
      severity: critical
    annotations:
      summary: "No timeline events observed"
      description: "No timeline events have been observed in the last 2 hours. This might indicate an issue with event logging."
